{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec919af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a07e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Labeling!\n",
      "20000\n",
      "Labeling!\n",
      "30000\n",
      "40000\n",
      "Labeling!\n",
      "50000\n",
      "Labeling!\n"
     ]
    }
   ],
   "source": [
    "def proc_texts(texts_tup, out):\n",
    "    print(\"Labeling!\")\n",
    "    for doc, meta in nlp.pipe(texts_tup, n_process=6, batch_size=1024, as_tuples=True):\n",
    "        #Write metadata + lemmatized text\n",
    "        meta['lemmatized'] = [t.lemma_ for t in doc]\n",
    "\n",
    "        json.dump(meta, out)\n",
    "        out.write('\\n')\n",
    "        \n",
    "def get_meta(val, typ):\n",
    "    if typ == 'post':\n",
    "        return {'sentiment': val['body_sentiment'], 'type': 'post', 'id': val['id']}\n",
    "    \n",
    "    if typ == 'comment':\n",
    "        return {'sentiment': val['text_sentiment'], 'type': 'comment', 'id': val['id']}\n",
    "\n",
    "    return None\n",
    "\n",
    "def lemmatize_all_text():\n",
    "    doc_acc = []\n",
    "    inp = open('./data/analyzed.jsonl', 'r')\n",
    "    outp = open('lemmatized.jsonl', 'w')\n",
    "\n",
    "    count = 0\n",
    "    for line in inp:\n",
    "        count += 1\n",
    "        if count % 10000 == 0: print(count)\n",
    "        if count == 50000: break\n",
    "\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        #Append relevant information\n",
    "        doc_acc.append((obj['body'], get_meta(obj, 'post')))\n",
    "        for c in obj['comments']: doc_acc.append((c['text'], get_meta(c, 'comment')))\n",
    "\n",
    "        #Append answer data too\n",
    "        for a in obj['answers']:\n",
    "            doc_acc.append((a['body'], get_meta(a, 'post')))\n",
    "            for c in a['comments']: doc_acc.append((c['text'], get_meta(c, 'comment')))\n",
    "        \n",
    "        #Process when limit reached\n",
    "        if len(doc_acc) > 100000:\n",
    "            proc_texts(doc_acc, outp)\n",
    "            doc_acc.clear()\n",
    "\n",
    "    #Process remaining\n",
    "    if doc_acc:\n",
    "        proc_texts(doc_acc, outp)\n",
    "        doc_acc.clear()\n",
    "\n",
    "    inp.close()\n",
    "    outp.close()\n",
    "\n",
    "lemmatize_all_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a270b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m idf = {k: np.log(cnt / v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwords\u001b[49m.items()}\n",
      "\u001b[31mNameError\u001b[39m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "idf = {k: np.log(cnt / v) for k,v in words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082813d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = {k: v * idf[k] for k,v in words_neg.items() if v < .2 * len(idf)}\n",
    "\n",
    "res = sorted([(v,k) for k,v in tfidf.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43024006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(14935.243903011851), 'sorry'),\n",
       " (np.float64(12178.577485153597), '?'),\n",
       " (np.float64(11405.603043628997), ','),\n",
       " (np.float64(10069.640926295946), 'get'),\n",
       " (np.float64(9514.550586144658), '.'),\n",
       " (np.float64(9308.402822520187), 'work'),\n",
       " (np.float64(9012.637742251838), 'code'),\n",
       " (np.float64(8911.19730046689), 'try'),\n",
       " (np.float64(8565.49284129855), ')'),\n",
       " (np.float64(8486.399440300856), 'use'),\n",
       " (np.float64(8325.380044418504), ':'),\n",
       " (np.float64(8302.33116330113), '('),\n",
       " (np.float64(8272.395202729454), '...'),\n",
       " (np.float64(8094.236601026941), 'wrong'),\n",
       " (np.float64(7742.389025783118), '-'),\n",
       " (np.float64(7696.56460091896), 'know'),\n",
       " (np.float64(7535.769053293209), 'error'),\n",
       " (np.float64(7457.588975467936), 'like'),\n",
       " (np.float64(7393.721948515933), 'problem'),\n",
       " (np.float64(7266.920090546608), '\"'),\n",
       " (np.float64(6894.1033223645145), 'understand'),\n",
       " (np.float64(6794.893877979243), 'question'),\n",
       " (np.float64(6760.265395777865), '@user'),\n",
       " (np.float64(6680.872522070391), 'want'),\n",
       " (np.float64(6601.821580942061), ' '),\n",
       " (np.float64(6484.471567575993), 'still'),\n",
       " (np.float64(6316.551695419404), 'make'),\n",
       " (np.float64(6299.768033112668), '!'),\n",
       " (np.float64(6267.835694989608), 'bad'),\n",
       " (np.float64(6097.4740833572705), 'would'),\n",
       " (np.float64(6087.863812173958), 'one'),\n",
       " (np.float64(5988.6569625072725), 'think'),\n",
       " (np.float64(5884.512069863232), 'seem'),\n",
       " (np.float64(5720.659801449311), 'really'),\n",
       " (np.float64(5632.5055804726), 'see'),\n",
       " (np.float64(5562.15016854076), 'way'),\n",
       " (np.float64(5488.215191429082), 'time'),\n",
       " (np.float64(5438.735625972298), '\\n'),\n",
       " (np.float64(5324.917140936858), 'look'),\n",
       " (np.float64(5310.935641987451), 'go'),\n",
       " (np.float64(5294.5884790967575), 'find'),\n",
       " (np.float64(5270.302754922927), 'need'),\n",
       " (np.float64(5166.263724414847), 'something'),\n",
       " (np.float64(5069.37002322285), ':('),\n",
       " (np.float64(5041.408423999631), 'file'),\n",
       " (np.float64(4839.201366548822), 'also'),\n",
       " (np.float64(4837.359752322031), 'sure'),\n",
       " (np.float64(4730.851656975269), 'could'),\n",
       " (np.float64(4730.776354855951), 'say'),\n",
       " (np.float64(4631.101831540995), 'add')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
